{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWZ6ehhrmcvkUEHLHLLp4n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dasaparnabme07/Reinforcement-Learning-Based-Prosthetic-Hand-Control/blob/main/RL_Based_Prosthetic_Hand_Control.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Fv9vbsZTkV6n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prosthetic hand environment (Actor-Critic)\n",
        "class ProstheticHandEnv(gym.Env):\n",
        "    def __init__(self, emg_features, target_actions):\n",
        "        super(ProstheticHandEnv, self).__init__()\n",
        "        self.emg_features = emg_features\n",
        "        self.target_actions = target_actions\n",
        "        self.num_samples = len(emg_features)\n",
        "\n",
        "        # Define observation and action spaces\n",
        "        self.observation_space = spaces.Box(low=-1.0, high=1.0, shape=(emg_features.shape[1],), dtype=np.float32)\n",
        "        self.action_space = spaces.Discrete(len(np.unique(target_actions)))\n",
        "        self.current_index = 0\n",
        "        def reset(self):\n",
        "          self.current_index = np.random.randint(0, self.num_samples)#Reset environment\n",
        "          return self.emg_features[self.current_index]\n",
        "\n",
        "    def step(self, action):\n",
        "        #Apply action, compute reward, and update state\n",
        "        target_action = self.target_actions[self.current_index]\n",
        "        reward = 1.0 if action == target_action else -1.0\n",
        "        self.current_index = (self.current_index + 1) % self.num_samples\n",
        "        done = self.current_index == 0\n",
        "        return self.emg_features[self.current_index], reward, done, {}\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        print(f\"Current step: {self.current_index}\")"
      ],
      "metadata": {
        "id": "oqOPLNFS4uRl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Actor and Critic networks\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(Actor, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "id": "9XpHWPYy5tlH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Actor-Critic agent\n",
        "class ActorCriticAgent:\n",
        "    def __init__(self, input_dim, action_dim, lr=0.001, gamma=0.99):\n",
        "        self.actor = Actor(input_dim, action_dim)\n",
        "        self.critic = Critic(input_dim)\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr)\n",
        "        self.gamma = gamma\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "        probs = self.actor(state_tensor)\n",
        "        dist = torch.distributions.Categorical(probs)\n",
        "        action = dist.sample()\n",
        "        return action.item(), dist.log_prob(action)\n",
        "\n",
        "    def update(self, state, reward, next_state, log_prob, done):\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "        next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
        "\n",
        "        # Calculate targets\n",
        "        target = reward + (1 - done) * self.gamma * self.critic(next_state_tensor).item()\n",
        "        target = torch.tensor(target).unsqueeze(0)\n",
        "\n",
        "        # Critic loss\n",
        "        value = self.critic(state_tensor)\n",
        "        critic_loss = self.loss_fn(value, target)\n",
        "\n",
        "        # Actor loss\n",
        "        advantage = target - value.detach()\n",
        "        actor_loss = -log_prob * advantage\n",
        "\n",
        "        # Update actor and critic\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        critic_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        self.critic_optimizer.step()"
      ],
      "metadata": {
        "id": "Su4lVmHQ50ZY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset (replace 'emg_data.csv' with your file)\n",
        "import scipy.io\n",
        "import pandas as pd\n",
        "data= scipy.io.loadmat('/content/S1_A1_E1.mat')\n",
        "emg_signals = data.iloc[:, :-1].values  # EMG channels\n",
        "target_actions = data.iloc[:, -1].values  # Target actions\n",
        "\n",
        "# Extract features\n",
        "sampling_rate = 1000  # Adjust based on your dataset\n",
        "emg_features = np.array([extract_features(signal, sampling_rate) for signal in emg_signals])"
      ],
      "metadata": {
        "id": "nl0LaEG855p4",
        "outputId": "e61ecbaf-2c29-4201-c61c-110d51e87d6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'dict' object has no attribute 'iloc'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-79c26d391c45>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/S1_A1_E1.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0memg_signals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m  \u001b[0;31m# EMG channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtarget_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m  \u001b[0;31m# Target actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'iloc'"
          ]
        }
      ]
    }
  ]
}